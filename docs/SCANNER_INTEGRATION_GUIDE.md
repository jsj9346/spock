# Scanner.py Integration Guide - MarketFilterManager

**Purpose**: Integrate MarketFilterManager into scanner.py for Stage 0 filtering
**Estimated Time**: 3-4 hours
**Status**: Implementation Guide

---

## Current State Analysis

### Existing scanner.py Structure (534 lines)
```python
class StockScanner:
    def __init__(self, db_path: str = 'data/spock_local.db'):
        self.db_path = db_path
        self.krx_api_key = os.getenv('KRX_API_KEY')
        self.sources = self._initialize_sources()  # KRX, pykrx, FDR

    def scan_all_tickers(self, force_refresh: bool = False) -> List[Dict]:
        # 1. Check cache (line 270-274)
        # 2. Try each data source sequentially (line 276-297)
        # 3. Apply Spock filters (line 287) â† TARGET FOR REPLACEMENT
        # 4. Save to cache (line 290)

    def _apply_spock_filters(self, tickers: List[Dict]) -> List[Dict]:
        # âŒ OLD: Hardcoded Korean market filters (line 302-335)
        # - MIN_MARKET_CAP = 10B KRW (line 309)
        # - KONEX exclusion (line 319)
        # - ETF/ETN exclusion (line 323-325)
        #
        # âœ… NEW: Use MarketFilterManager
        pass

    def _save_to_cache(self, tickers: List[Dict], source: str):
        # âŒ OLD: Saves to 'tickers' table (line 395-484)
        #
        # âœ… NEW: Save to 'filter_cache_stage0' table
        pass
```

---

## Integration Strategy

### Step 1: Add MarketFilterManager Import

**Location**: Line ~20 (after existing imports)

```python
# === EXISTING IMPORTS ===
import os
import sys
import time
import logging
import sqlite3
import requests
from typing import List, Dict, Optional
from datetime import datetime, timedelta

# === NEW IMPORT (ADD THIS) ===
from modules.market_filter_manager import MarketFilterManager, FilterResult
```

---

### Step 2: Update StockScanner.__init__()

**Location**: Line ~232-237

**BEFORE**:
```python
class StockScanner:
    def __init__(self, db_path: str = 'data/spock_local.db'):
        self.db_path = db_path
        self.krx_api_key = os.getenv('KRX_API_KEY')  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ

        # ë°ì´í„° ì†ŒìŠ¤ ì´ˆê¸°í™”
        self.sources = self._initialize_sources()
```

**AFTER**:
```python
class StockScanner:
    def __init__(self, db_path: str = 'data/spock_local.db', region: str = 'KR'):
        self.db_path = db_path
        self.region = region.upper()  # 'KR', 'US', etc.
        self.krx_api_key = os.getenv('KRX_API_KEY')

        # NEW: MarketFilterManager ì´ˆê¸°í™”
        self.filter_manager = MarketFilterManager(
            config_dir='config/market_filters'
        )

        # Verify region config exists
        if not self.filter_manager.has_config(self.region):
            raise ValueError(
                f"No configuration found for region: {self.region}. "
                f"Available: {self.filter_manager.get_supported_regions()}"
            )

        # ë°ì´í„° ì†ŒìŠ¤ ì´ˆê¸°í™”
        self.sources = self._initialize_sources()
```

---

### Step 3: Replace _apply_spock_filters() Method

**Location**: Line ~302-335

**BEFORE**:
```python
def _apply_spock_filters(self, tickers: List[Dict]) -> List[Dict]:
    """
    Spock í•„í„°ë§ ê¸°ì¤€ ì ìš©
    - ì‹œê°€ì´ì•¡ >= 100ì–µì›
    - ETF/ETN ì œì™¸
    - KONEX ì œì™¸
    """
    MIN_MARKET_CAP = 10_000_000_000  # 100ì–µì›

    filtered = []
    for ticker_data in tickers:
        # ì‹œê°€ì´ì•¡ í•„í„° (ìˆëŠ” ê²½ìš°ë§Œ)
        market_cap = ticker_data.get('market_cap', 0)
        if market_cap > 0 and market_cap < MIN_MARKET_CAP:
            continue

        # KONEX ì œì™¸
        if ticker_data.get('market') == 'KONEX':
            continue

        # ETF/ETN ì œì™¸
        name = ticker_data.get('name', '')
        if any(keyword in name for keyword in ['ETF', 'ETN', 'KODEX', 'TIGER', 'KBSTAR', 'ARIRANG']):
            continue

        # ê¸°ë³¸ í•„ë“œ ì¶”ê°€
        ticker_data['region'] = 'KR'
        ticker_data['currency'] = 'KRW'
        ticker_data['is_active'] = True

        filtered.append(ticker_data)

    logger.info(f"ğŸ“Š í•„í„°ë§: {len(tickers)} â†’ {len(filtered)}ê°œ ì¢…ëª© (ì‹œê°€ì´ì•¡ 100ì–µ ì´ìƒ)")
    return filtered
```

**AFTER**:
```python
def _apply_stage0_filter(self, tickers: List[Dict]) -> List[Dict]:
    """
    Stage 0: Basic Market Filter (MarketFilterManager ì‚¬ìš©)

    Args:
        tickers: Raw ticker list from data source

    Returns:
        Filtered ticker list with normalized data
    """
    logger.info(f"ğŸ” Stage 0 í•„í„° ì ìš© ì¤‘... (ì…ë ¥: {len(tickers)}ê°œ)")

    filtered = []
    passed_count = 0
    filter_reasons = {}

    for ticker_data in tickers:
        ticker = ticker_data.get('ticker', 'UNKNOWN')

        # Prepare data for filter (must have local currency values)
        filter_input = {
            'ticker': ticker,
            'name': ticker_data.get('name', ''),
            'asset_type': self._classify_asset_type(ticker_data.get('name', '')),
            'market': ticker_data.get('market', 'UNKNOWN'),
            'market_cap_local': ticker_data.get('market_cap', 0),
            'trading_value_local': ticker_data.get('trading_value', 0),
            'price_local': ticker_data.get('close_price', 0),
            'market_warn_code': ticker_data.get('market_warn_code', '00'),
            'is_delisting': ticker_data.get('is_delisting', False)
        }

        # Apply filter using MarketFilterManager
        result: FilterResult = self.filter_manager.apply_stage0_filter(
            region=self.region,
            ticker_data=filter_input
        )

        if result.passed:
            # Merge normalized data back into ticker_data
            ticker_data.update(result.normalized_data)
            ticker_data['region'] = self.region
            ticker_data['stage0_passed'] = True
            filtered.append(ticker_data)
            passed_count += 1
        else:
            # Track filter reasons for reporting
            reason = result.reason
            filter_reasons[reason] = filter_reasons.get(reason, 0) + 1

    # Log filter statistics
    reduction_rate = (len(tickers) - passed_count) / len(tickers) * 100
    logger.info(
        f"âœ… Stage 0 í•„í„° ì™„ë£Œ: {len(tickers)} â†’ {passed_count}ê°œ "
        f"({reduction_rate:.1f}% ê°ì†Œ)"
    )

    # Log top filter reasons
    if filter_reasons:
        top_reasons = sorted(
            filter_reasons.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        logger.info(f"ğŸ“Š í•„í„°ë§ ì´ìœ  (ìƒìœ„ 5ê°œ):")
        for reason, count in top_reasons:
            logger.info(f"   - {reason}: {count}ê°œ")

    return filtered
```

---

### Step 4: Update scan_all_tickers() to Use New Filter

**Location**: Line ~287

**BEFORE**:
```python
# í•„í„°ë§ ì ìš©
filtered = self._apply_spock_filters(tickers)
```

**AFTER**:
```python
# Stage 0 í•„í„° ì ìš© (MarketFilterManager)
filtered = self._apply_stage0_filter(tickers)
```

---

### Step 5: Replace _save_to_cache() Method

**Location**: Line ~395-484

**BEFORE**:
```python
def _save_to_cache(self, tickers: List[Dict], source: str):
    """SQLite ìºì‹œì— ì¢…ëª© ì €ì¥ (tickers + ticker_fundamentals ë¶„ë¦¬)"""
    try:
        # ... saves to 'tickers' and 'ticker_fundamentals' tables
        pass
```

**AFTER**:
```python
def _save_to_filter_cache(self, tickers: List[Dict], source: str):
    """
    Stage 0 í•„í„° ê²°ê³¼ë¥¼ filter_cache_stage0 í…Œì´ë¸”ì— ì €ì¥

    Args:
        tickers: Filtered ticker list with normalized data
        source: Data source name
    """
    try:
        # DB ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # ê¸°ì¡´ filter_cache_stage0 ë°ì´í„° ì‚­ì œ (í•´ë‹¹ regionë§Œ)
        cursor.execute(
            "DELETE FROM filter_cache_stage0 WHERE region = ?",
            (self.region,)
        )

        # ìƒˆ ë°ì´í„° ì‚½ì…
        now = datetime.now().isoformat()
        today = datetime.now().strftime("%Y-%m-%d")
        inserted_count = 0

        for ticker_data in tickers:
            ticker = ticker_data['ticker']

            # filter_cache_stage0 í…Œì´ë¸”ì— ì‚½ì…
            cursor.execute("""
                INSERT OR REPLACE INTO filter_cache_stage0
                (
                    ticker, region, name, exchange,
                    market_cap_krw, trading_value_krw, current_price_krw,
                    market_cap_local, trading_value_local, current_price_local,
                    currency, exchange_rate_to_krw, exchange_rate_date, exchange_rate_source,
                    market_warn_code, is_delisting,
                    filter_date, stage0_passed, created_at, last_updated
                )
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                ticker,
                self.region,
                ticker_data.get('name'),
                ticker_data.get('market'),
                # KRW normalized values
                ticker_data.get('market_cap_krw'),
                ticker_data.get('trading_value_krw'),
                ticker_data.get('current_price_krw'),
                # Local currency values
                ticker_data.get('market_cap_local'),
                ticker_data.get('trading_value_local'),
                ticker_data.get('current_price_local'),
                # Exchange rate metadata
                ticker_data.get('currency', 'KRW'),
                ticker_data.get('exchange_rate_to_krw', 1.0),
                ticker_data.get('exchange_rate_date', today),
                ticker_data.get('exchange_rate_source', 'fixed'),
                # Market-specific fields
                ticker_data.get('market_warn_code'),
                ticker_data.get('is_delisting', False),
                # Filter metadata
                today,
                True,  # stage0_passed
                now,
                now
            ))

            inserted_count += 1

        conn.commit()
        conn.close()

        logger.info(
            f"ğŸ’¾ filter_cache_stage0 ì €ì¥ ì™„ë£Œ: {inserted_count}ê°œ ì¢…ëª© "
            f"(Region: {self.region}, Source: {source})"
        )

    except Exception as e:
        logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
        raise
```

---

### Step 6: Update _load_from_cache() Method

**Location**: Line ~337-393

**BEFORE**:
```python
def _load_from_cache(self) -> Optional[List[Dict]]:
    """SQLite ìºì‹œì—ì„œ ì¢…ëª© ë¡œë“œ (24ì‹œê°„ ì´ë‚´)"""
    # ... loads from 'tickers' table
```

**AFTER**:
```python
def _load_from_filter_cache(self) -> Optional[List[Dict]]:
    """
    filter_cache_stage0 í…Œì´ë¸”ì—ì„œ ìºì‹œ ë¡œë“œ (TTL í™•ì¸)

    Returns:
        Cached ticker list or None if cache expired/empty
    """
    try:
        if not os.path.exists(self.db_path):
            return None

        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        # ìµœê·¼ ì—…ë°ì´íŠ¸ í™•ì¸ (regionë³„)
        cursor.execute("""
            SELECT MAX(last_updated) as last_update
            FROM filter_cache_stage0
            WHERE region = ? AND stage0_passed = 1
        """, (self.region,))

        result = cursor.fetchone()

        if not result or not result['last_update']:
            conn.close()
            return None

        last_update = datetime.fromisoformat(result['last_update'])
        age_hours = (datetime.now() - last_update).total_seconds() / 3600

        # TTL check (1 hour during market hours, 24 hours after-hours)
        # Simplified: Use 24-hour TTL for now
        if age_hours > 24:
            logger.info(f"â° ìºì‹œ ë§Œë£Œ ({age_hours:.1f}ì‹œê°„ ê²½ê³¼)")
            conn.close()
            return None

        # Load cached tickers
        cursor.execute("""
            SELECT
                ticker,
                name,
                exchange as market,
                region,
                currency,
                market_cap_krw,
                trading_value_krw,
                current_price_krw,
                market_cap_local,
                trading_value_local,
                current_price_local,
                exchange_rate_to_krw,
                exchange_rate_date,
                exchange_rate_source,
                filter_date
            FROM filter_cache_stage0
            WHERE region = ? AND stage0_passed = 1
            ORDER BY market_cap_krw DESC
        """, (self.region,))

        tickers = [dict(row) for row in cursor.fetchall()]
        conn.close()

        logger.info(
            f"âœ… ìºì‹œ íˆíŠ¸: {len(tickers)}ê°œ ì¢…ëª© ë¡œë“œ "
            f"({age_hours:.1f}ì‹œê°„ ì „ ë°ì´í„°, Region: {self.region})"
        )
        return tickers

    except Exception as e:
        logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
```

---

### Step 7: Update scan_all_tickers() Method

**Location**: Line ~259-300

**BEFORE**:
```python
def scan_all_tickers(self, force_refresh: bool = False) -> List[Dict]:
    # Layer 0: Cache í™•ì¸ (ìµœìš°ì„ )
    if not force_refresh:
        cached = self._load_from_cache()
        if cached:
            logger.info(f"âœ… [Cache] {len(cached)}ê°œ ì¢…ëª© ë¡œë“œ (ìºì‹œ ì‚¬ìš©)")
            return cached

    # Layer 1-4: ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
    for source_name, source_api in self.sources:
        try:
            # ... fetch tickers
            # í•„í„°ë§ ì ìš©
            filtered = self._apply_spock_filters(tickers)

            # Cache ì €ì¥
            self._save_to_cache(filtered, source=source_name)
            # ...
```

**AFTER**:
```python
def scan_all_tickers(self, force_refresh: bool = False) -> List[Dict]:
    """
    ì „ì²´ ì¢…ëª© ìŠ¤ìº” + Stage 0 í•„í„° ì ìš©

    Args:
        force_refresh: ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ê°±ì‹ 

    Returns:
        Stage 0 í•„í„°ë¥¼ í†µê³¼í•œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸
    """
    # Layer 0: Cache í™•ì¸ (filter_cache_stage0)
    if not force_refresh:
        cached = self._load_from_filter_cache()  # â† CHANGED
        if cached:
            logger.info(f"âœ… [Cache] {len(cached)}ê°œ ì¢…ëª© ë¡œë“œ (Region: {self.region})")
            return cached

    # Layer 1-4: ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
    for source_name, source_api in self.sources:
        try:
            logger.info(f"ğŸ”„ [{source_name}] ì¢…ëª© ì¡°íšŒ ì‹œë„...")
            tickers = source_api.get_stock_list()

            if not tickers:
                logger.warning(f"âš ï¸ [{source_name}] ë°ì´í„° ì—†ìŒ, ë‹¤ìŒ ì†ŒìŠ¤ ì‹œë„")
                continue

            # Stage 0 í•„í„° ì ìš© (MarketFilterManager)
            filtered = self._apply_stage0_filter(tickers)  # â† CHANGED

            # Cache ì €ì¥ (filter_cache_stage0)
            self._save_to_filter_cache(filtered, source=source_name)  # â† CHANGED

            logger.info(f"âœ… [{source_name}] {len(filtered)}ê°œ ì¢…ëª© ìŠ¤ìº” ì™„ë£Œ")
            return filtered

        except Exception as e:
            logger.error(f"âŒ [{source_name}] ì‹¤íŒ¨: {e}")
            continue

    # ëª¨ë“  ì†ŒìŠ¤ ì‹¤íŒ¨
    raise Exception("âŒ ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ì‹¤íŒ¨. ë„¤íŠ¸ì›Œí¬ í™•ì¸ í•„ìš”.")
```

---

### Step 8: Update CLI Arguments

**Location**: Line ~500-514

**BEFORE**:
```python
parser = argparse.ArgumentParser(description='êµ­ë‚´ì£¼ì‹ ì¢…ëª© ìŠ¤ìºë„ˆ')
parser.add_argument('--force-refresh', action='store_true', help='ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ê°±ì‹ ')
parser.add_argument('--db-path', default='data/spock_local.db', help='SQLite DB ê²½ë¡œ')
parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
```

**AFTER**:
```python
parser = argparse.ArgumentParser(description='ë‹¤ì¤‘ ì‹œì¥ ì¢…ëª© ìŠ¤ìºë„ˆ (Multi-Market Scanner)')
parser.add_argument('--force-refresh', action='store_true', help='ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œ ê°±ì‹ ')
parser.add_argument('--db-path', default='data/spock_local.db', help='SQLite DB ê²½ë¡œ')
parser.add_argument('--region', default='KR', help='ì‹œì¥ ì§€ì—­ ì½”ë“œ (KR, US, HK, CN, JP, VN)')
parser.add_argument('--debug', action='store_true', help='ë””ë²„ê·¸ ëª¨ë“œ')
```

**Update scanner initialization** (Line ~514):

**BEFORE**:
```python
scanner = StockScanner(db_path=args.db_path)
```

**AFTER**:
```python
scanner = StockScanner(db_path=args.db_path, region=args.region)
```

---

## Testing Plan

### Unit Tests
```bash
# Test MarketFilterManager integration
python3 -c "
from modules.scanner import StockScanner
scanner = StockScanner(region='KR')
print('âœ… Scanner initialized with MarketFilterManager')
print(f'Region: {scanner.region}')
print(f'Filter config loaded: {scanner.filter_manager.has_config(\"KR\")}')
"
```

### Integration Test
```bash
# Test full scan with filtering
python3 modules/scanner.py --force-refresh --region KR --debug

# Expected output:
# ğŸ”„ [KRX Data API] ì¢…ëª© ì¡°íšŒ ì‹œë„...
# âœ… [KRX Data API] 2,500ê°œ ì¢…ëª© ì¡°íšŒ
# ğŸ” Stage 0 í•„í„° ì ìš© ì¤‘... (ì…ë ¥: 2,500ê°œ)
# âœ… Stage 0 í•„í„° ì™„ë£Œ: 2,500 â†’ 600ê°œ (76.0% ê°ì†Œ)
# ğŸ“Š í•„í„°ë§ ì´ìœ  (ìƒìœ„ 5ê°œ):
#    - market_cap_too_low: 1,200ê°œ
#    - etf: 300ê°œ
#    - price_too_low: 200ê°œ
# ğŸ’¾ filter_cache_stage0 ì €ì¥ ì™„ë£Œ: 600ê°œ ì¢…ëª© (Region: KR)
```

---

## Validation Checklist

### Functional Tests
- [ ] MarketFilterManager initializes correctly
- [ ] Korea config (kr_filter_config.yaml) loads successfully
- [ ] Stage 0 filter applies thresholds correctly
- [ ] filter_cache_stage0 table saves data properly
- [ ] Cache TTL works (24-hour expiration)
- [ ] Force refresh bypasses cache
- [ ] Filter reasons logged correctly

### Performance Tests
- [ ] Scan completes in < 10 seconds (Stage 0 only)
- [ ] Database queries use indexes (check EXPLAIN QUERY PLAN)
- [ ] Memory usage stays < 500MB

### Error Handling Tests
- [ ] Handles missing config file gracefully
- [ ] Handles database connection errors
- [ ] Handles malformed ticker data
- [ ] Logs errors appropriately

---

## Migration Path (Safe Rollout)

### Option 1: Feature Flag (Recommended)
```python
# Add flag to enable/disable new filter
USE_MARKET_FILTER_MANAGER = os.getenv('USE_MARKET_FILTER_MANAGER', 'true').lower() == 'true'

if USE_MARKET_FILTER_MANAGER:
    filtered = self._apply_stage0_filter(tickers)  # New
else:
    filtered = self._apply_spock_filters(tickers)  # Old
```

### Option 2: Parallel Validation
```python
# Run both filters and compare results
old_filtered = self._apply_spock_filters(tickers)
new_filtered = self._apply_stage0_filter(tickers)

# Log differences
diff = set(old_filtered) - set(new_filtered)
if diff:
    logger.warning(f"âš ï¸ Filter mismatch: {len(diff)} tickers differ")
```

---

## Estimated Implementation Time

| Step | Description | Time |
|------|-------------|------|
| 1-2 | Imports + __init__() update | 30 min |
| 3-4 | Replace _apply_spock_filters() | 1 hour |
| 5-6 | Replace cache methods | 1 hour |
| 7-8 | Update scan_all_tickers() + CLI | 30 min |
| Testing | Unit + integration tests | 1 hour |
| **TOTAL** | **Full integration** | **4 hours** |

---

## Next Steps After Integration

1. **Test with Real Data**: Run scanner with actual KOSPI/KOSDAQ data
2. **Validate Filter Results**: Compare old vs new filter counts
3. **Performance Benchmark**: Measure execution time
4. **Proceed to Task 5**: Create stock_pre_filter.py (Stage 1)

---

**End of Integration Guide**
