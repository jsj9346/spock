# Grafana Alert Rules for PostgreSQL Monitoring
#
# Usage:
#   1. Copy this file to Grafana provisioning/alerting/rules directory
#   2. Reload Grafana configuration
#   3. Alerts will be automatically created
#
# Documentation: https://grafana.com/docs/grafana/latest/alerting/

groups:
  - name: PostgreSQL Database Alerts
    interval: 1m
    rules:
      # ===================================================================
      # Critical Alerts
      # ===================================================================

      - alert: DatabaseDown
        expr: up{job="postgres-metrics"} == 0
        for: 1m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "PostgreSQL metrics collector is down"
          description: "The PostgreSQL metrics collector has been down for more than 1 minute."
          runbook: "Check if the metrics server is running: ps aux | grep postgres_metrics"

      - alert: DatabaseSizeCritical
        expr: postgres_database_size_bytes > 100 * 1024 * 1024 * 1024  # 100GB
        for: 5m
        labels:
          severity: critical
          component: storage
        annotations:
          summary: "Database size exceeds 100GB"
          description: "Database {{ $labels.database }} size is {{ $value | humanize1024 }}B"
          runbook: "Review data retention policies and consider archiving old data"

      - alert: DuplicateRecordsDetected
        expr: postgres_duplicate_records_count > 0
        for: 5m
        labels:
          severity: critical
          component: data_quality
        annotations:
          summary: "Duplicate records detected in {{ $labels.table }}"
          description: "{{ $value }} duplicate records found in {{ $labels.table }}"
          runbook: "Run: python3 scripts/data_quality_checks.py --comprehensive"

      - alert: QueryErrorRateHigh
        expr: rate(postgres_query_errors_total[5m]) > 1
        for: 5m
        labels:
          severity: critical
          component: queries
        annotations:
          summary: "High query error rate"
          description: "Query errors {{ $labels.error_type }} at {{ $value | humanize }} errors/sec"
          runbook: "Check PostgreSQL logs for error details"

      # ===================================================================
      # Warning Alerts
      # ===================================================================

      - alert: DataStale
        expr: postgres_data_freshness_seconds{data_type="ohlcv"} > 604800  # 7 days
        for: 1h
        labels:
          severity: warning
          component: data_quality
        annotations:
          summary: "Stale data detected for {{ $labels.region }}"
          description: "{{ $labels.region }} OHLCV data is {{ $value | humanizeDuration }} old"
          runbook: "Check data collection scripts: python3 modules/kis_data_collector.py"

      - alert: MissingDatesHigh
        expr: postgres_missing_dates_count > 50
        for: 15m
        labels:
          severity: warning
          component: data_quality
        annotations:
          summary: "High number of missing dates in {{ $labels.region }}"
          description: "{{ $value }} date gaps detected (>3 days)"
          runbook: "Run: python3 scripts/data_quality_checks.py --region {{ $labels.region }}"

      - alert: SlowQueriesHigh
        expr: rate(postgres_slow_queries_total[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
          component: performance
        annotations:
          summary: "High slow query rate on {{ $labels.table }}"
          description: "Slow queries (>1s) at {{ $value | humanize }} queries/sec"
          runbook: "Run: python3 scripts/benchmark_postgres_performance.py --comprehensive"

      - alert: IndexHitRateLow
        expr: postgres_index_hit_rate < 0.9
        for: 15m
        labels:
          severity: warning
          component: performance
        annotations:
          summary: "Low index hit rate on {{ $labels.table }}"
          description: "Index hit rate is {{ $value | humanizePercentage }} (target: >90%)"
          runbook: "Consider adding indexes or optimizing queries for {{ $labels.table }}"

      - alert: BacktestQuerySlow
        expr: histogram_quantile(0.95, rate(postgres_backtest_query_seconds_bucket[5m])) > 1.0
        for: 10m
        labels:
          severity: warning
          component: performance
        annotations:
          summary: "Backtest queries are slow (P95 > 1s)"
          description: "95th percentile query time: {{ $value }}s for {{ $labels.time_range }}"
          runbook: "Check query plan: EXPLAIN ANALYZE SELECT ... FROM ohlcv_data"

      - alert: ActiveConnectionsHigh
        expr: sum(postgres_active_connections) > 50
        for: 10m
        labels:
          severity: warning
          component: connections
        annotations:
          summary: "High number of active connections"
          description: "{{ $value }} active connections (consider connection pooling)"
          runbook: "Review active queries: SELECT * FROM pg_stat_activity"

      - alert: HypertableChunksHigh
        expr: postgres_hypertable_chunks > 1000
        for: 30m
        labels:
          severity: warning
          component: timescaledb
        annotations:
          summary: "High chunk count for {{ $labels.hypertable }}"
          description: "{{ $value }} chunks (consider adjusting chunk_time_interval)"
          runbook: "Review chunk size: SELECT show_chunks('{{ $labels.hypertable }}')"

      # ===================================================================
      # Info Alerts
      # ===================================================================

      - alert: CompressionNotEnabled
        expr: postgres_compressed_chunks == 0 and postgres_hypertable_chunks > 100
        for: 24h
        labels:
          severity: info
          component: timescaledb
        annotations:
          summary: "Compression not enabled for {{ $labels.hypertable }}"
          description: "{{ $labels.hypertable }} has {{ postgres_hypertable_chunks }} uncompressed chunks"
          runbook: "Enable compression policy for space savings"

      - alert: TableSizeGrowing
        expr: rate(postgres_table_size_bytes[24h]) > 1024 * 1024 * 100  # 100MB/day
        for: 7d
        labels:
          severity: info
          component: storage
        annotations:
          summary: "{{ $labels.table }} is growing rapidly"
          description: "Growth rate: {{ $value | humanize1024 }}B/day"
          runbook: "Monitor for expected growth or implement retention policies"

  - name: Data Quality Alerts
    interval: 5m
    rules:
      - alert: OHLCVPriceInconsistency
        expr: postgres_null_values_count{table="ohlcv_data",column=~"open|high|low|close"} > 0
        for: 5m
        labels:
          severity: warning
          component: data_quality
        annotations:
          summary: "NULL values in OHLCV prices"
          description: "{{ $value }} NULL values in {{ $labels.column }}"
          runbook: "Run: python3 scripts/data_quality_checks.py --comprehensive"

      - alert: TickerCountDropped
        expr: (postgres_total_tickers offset 1h) - postgres_total_tickers > 10
        for: 30m
        labels:
          severity: warning
          component: data_integrity
        annotations:
          summary: "Significant drop in ticker count"
          description: "{{ $labels.region }}: Lost {{ $value }} tickers in past hour"
          runbook: "Check data ingestion pipeline and tickers table"

  - name: Backup Alerts
    interval: 24h
    rules:
      - alert: BackupMissing
        expr: time() - postgres_database_info{last_backup_time!=""} > 172800  # 2 days
        for: 1h
        labels:
          severity: critical
          component: backup
        annotations:
          summary: "Database backup is overdue"
          description: "Last backup was {{ $value | humanizeDuration }} ago"
          runbook: "Run: ./scripts/backup_postgres.sh"

# Alert routing configuration
route:
  receiver: 'default-receiver'
  group_by: ['alertname', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
      repeat_interval: 1h

    - match:
        severity: warning
      receiver: 'warning-alerts'
      repeat_interval: 4h

    - match:
        severity: info
      receiver: 'info-alerts'
      repeat_interval: 24h

# Alert receivers (configure based on your notification channels)
receivers:
  - name: 'default-receiver'
    # Add your notification channels here

  - name: 'critical-alerts'
    # PagerDuty, email, Slack, etc.
    # email_configs:
    #   - to: 'alerts@example.com'
    #     from: 'grafana@example.com'
    #     smarthost: 'smtp.example.com:587'

  - name: 'warning-alerts'
    # Slack, email, etc.

  - name: 'info-alerts'
    # Email, log aggregation, etc.
