import time
from functools import wraps
import re
# import psycopg2  # PostgreSQL ì˜ì¡´ì„± ì œê±° - SQLite ì‚¬ìš©
import os
from dotenv import load_dotenv
import functools
import requests
from datetime import datetime, date, timedelta
from typing import Dict, Optional
import logging
import sys
import json
import pytz
import pandas as pd
import psutil

# Import optimized monitor (commented out - module deleted)
# from optimized_data_monitor import get_optimized_monitor

# === ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” ===
_conversion_stats = {
    "total_calls": 0,
    "successful_conversions": 0,
    "conversion_failures": 0,
    "datetime_detections": 0
}

def safe_strftime(date_obj, format_str='%Y-%m-%d'):
    """
    ì•ˆì „í•œ datetime ë³€í™˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    
    ì¡°ê±´ë¶€ ì²˜ë¦¬:
    - pd.Timestampì¸ ê²½ìš°: ì§ì ‘ strftime ì‚¬ìš©
    - pd.DatetimeIndexì¸ ê²½ìš°: ì§ì ‘ strftime ì‚¬ìš©
    - ì •ìˆ˜í˜•ì¸ ê²½ìš°: pd.to_datetime()ìœ¼ë¡œ ë³€í™˜ í›„ strftime
    - None/NaNì¸ ê²½ìš°: ê¸°ë³¸ê°’ ë°˜í™˜
    
    Args:
        date_obj: ë³€í™˜í•  ë‚ ì§œ ê°ì²´ (datetime, pd.Timestamp, int, str ë“±)
        format_str (str): ë‚ ì§œ í¬ë§· ë¬¸ìì—´ (ê¸°ë³¸ê°’: '%Y-%m-%d')
        
    Returns:
        str: í¬ë§·ëœ ë‚ ì§œ ë¬¸ìì—´ ë˜ëŠ” ê¸°ë³¸ê°’
    """
    try:
        # None ë˜ëŠ” NaN ì²´í¬
        if date_obj is None or (hasattr(date_obj, 'isna') and date_obj.isna()):
            return "N/A"
        
        # pandas NaT ì²´í¬
        if pd.isna(date_obj):
            return "N/A"
        
        # ë¹ˆ ë¬¸ìì—´ ì²´í¬
        if isinstance(date_obj, str) and date_obj.strip() == "":
            return "N/A"
        
        # ë¦¬ìŠ¤íŠ¸, íŠœí”Œ, ë”•ì…”ë„ˆë¦¬ ë“± ì»¨í…Œì´ë„ˆ íƒ€ì… ì²´í¬
        if isinstance(date_obj, (list, tuple, dict)):
            return str(date_obj)
        
        # pandas Timestamp ê°ì²´ì¸ ê²½ìš° (ìš°ì„  ì²˜ë¦¬)
        if isinstance(date_obj, pd.Timestamp):
            return date_obj.strftime(format_str)
        
        # pandas DatetimeIndex ìš”ì†Œì¸ ê²½ìš°
        if hasattr(date_obj, '__class__') and 'pandas' in str(type(date_obj)):
            try:
                # pandas datetime-like ê°ì²´ ì²˜ë¦¬
                return pd.Timestamp(date_obj).strftime(format_str)
            except:
                pass
            
        # ì´ë¯¸ datetime ê°ì²´ì´ê±°ë‚˜ strftime ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš°
        if hasattr(date_obj, 'strftime'):
            return date_obj.strftime(format_str)
            
        # ì •ìˆ˜í˜•ì¸ ê²½ìš° (timestamp)
        if isinstance(date_obj, (int, float)):
            # Unix timestampë¡œ ê°€ì •í•˜ê³  ë³€í™˜ (ë‚˜ë…¸ì´ˆë„ ê³ ë ¤)
            if date_obj > 1e15:  # ë‚˜ë…¸ì´ˆ timestamp
                dt = pd.to_datetime(date_obj, unit='ns')
            elif date_obj > 1e10:  # ë°€ë¦¬ì´ˆ timestamp
                dt = pd.to_datetime(date_obj, unit='ms')
            else:  # ì´ˆ timestamp
                dt = pd.to_datetime(date_obj, unit='s')
            return dt.strftime(format_str)
            
        # ë¬¸ìì—´ì¸ ê²½ìš°
        if isinstance(date_obj, str):
            # ë¹ˆ ë¬¸ìì—´ ì¬í™•ì¸
            if date_obj.strip() == "":
                return "N/A"
            # ì´ë¯¸ í¬ë§·ëœ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if len(date_obj) >= 10 and '-' in date_obj:
                return date_obj[:10]  # YYYY-MM-DD ë¶€ë¶„ë§Œ ì¶”ì¶œ
            # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ datetimeìœ¼ë¡œ ë³€í™˜ ì‹œë„
            dt = pd.to_datetime(date_obj)
            return dt.strftime(format_str)
            
        # ê¸°íƒ€ ê²½ìš°: pandas to_datetimeìœ¼ë¡œ ë³€í™˜ ì‹œë„
        dt = pd.to_datetime(date_obj)
        return dt.strftime(format_str)
        
    except Exception as e:
        # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸í•œ ë¡œê·¸
        logger.debug(f"safe_strftime ë³€í™˜ ì‹¤íŒ¨ - ì…ë ¥ê°’: {date_obj} (íƒ€ì…: {type(date_obj)}), ì˜¤ë¥˜: {e}")
        
        # ëª¨ë“  ë³€í™˜ì´ ì‹¤íŒ¨í•œ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        try:
            result = str(date_obj)
            if result == "":
                return "N/A"
            return result[:10] if len(result) >= 10 else result
        except:
            return "Invalid Date"

def retry(max_attempts=3, initial_delay=0.5, backoff=2):
    """
    Decorator to retry a function on exception.
    :param max_attempts: maximum number of attempts (including first)
    :param initial_delay: initial wait time between retries
    :param backoff: multiplier for successive delays
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            delay = initial_delay
            for attempt in range(1, max_attempts + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts:
                        raise
                    time.sleep(delay)
                    delay *= backoff
        return wrapper
    return decorator

def compute_expected_phase(data):
    """
    Stan Weinstein Market Phases ë£°ì„ ë¶€ë¶„ì ìœ¼ë¡œ êµ¬í˜„í•œ ê°„ëµí™” ë¡œì§.
    :param data: dict with keys 'current_price', 'ma_50', 'ma_200', 'volume_change_7_30', 'r1', 'r2', 'r3', 's1'
    :return: int Phase number (1 to 4)
    """
    cp = float(data.get("current_price", 0))
    ma50 = float(data.get("ma_50", 0))
    ma200 = float(data.get("ma_200", 0))
    vol_chg = float(data.get("volume_change_7_30", 0))
    r1, r2, r3 = map(float, (data.get("r1", 0), data.get("r2", 0), data.get("r3", 0)))
    s1 = float(data.get("s1", 0))

    # Stage 4: ëª…ë°±í•œ í•˜ë½ì¶”ì„¸ (MA-50 < MA-200 ë° ê°€ê²© < MA-50)
    if ma50 < ma200 and cp < ma50:
        return 4
    # Stage 3: ì •ì ê¶Œ (ê³ ì  ê·¼ì²˜ + ê±°ë˜ëŸ‰ ê°ì†Œ)
    if cp >= r3 and vol_chg < 0:
        return 3
    # Stage 2: ìƒìŠ¹ì¶”ì„¸ (MA ë°°ì—´ + ê±°ë˜ëŸ‰ ì¦ê°€ + ê°€ê²© > MA-50)
    if ma50 > ma200 and vol_chg > 0 and cp > ma50:
        return 2
    # Stage 1: íš¡ë³´ê¶Œ (S1~R1 êµ¬ê°„, MA í‰íƒ„, ê±°ë˜ëŸ‰ í‰ì´)
    if s1 <= cp <= r1 and abs(ma50 - ma200) < (ma200 * 0.01) and abs(vol_chg) < 0.05:
        return 1
    # ê¸°ë³¸ì ìœ¼ë¡œ Stage 1ìœ¼ë¡œ ë°˜í™˜
    return 1


def validate_and_correct_phase(ticker, reported_phase, data, reason):
    """
    reported_phase: string like "Stage 2"
    :param ticker: str
    :param reported_phase: str
    :param data: dict of analysis data passed to GPT
    :param reason: original reason string from GPT
    :return: (corrected_phase_str, corrected_reason)
    """
    m = re.search(r'\d+', reported_phase or "")
    reported = int(m.group()) if m else None
    expected = compute_expected_phase(data)
    if reported is not None and reported != expected:
        print(f"âš ï¸ GPT hallucination for {ticker}: reported Stage {reported}, expected Stage {expected}")
        corrected = f"Stage {expected}"
        corrected_reason = f"{reason} | Phase forced to Stage {expected} by rule-based check"
        return corrected, corrected_reason
    return reported_phase, reason

# === ê³µí†µ ìƒìˆ˜ ===
MIN_KRW_ORDER = 10000  # ìµœì†Œ ë§¤ìˆ˜ ê¸ˆì•¡
MIN_KRW_SELL_ORDER = 5000  # ìµœì†Œ ë§¤ë„ ê¸ˆì•¡
TAKER_FEE_RATE = 0.00139  # ì—…ë¹„íŠ¸ KRW ë§ˆì¼“ Taker ìˆ˜ìˆ˜ë£Œ

# === í™˜ê²½ë³€ìˆ˜ ë¡œë”© ===
def load_env():
    load_dotenv()

# === DB ì—°ê²° í•¨ìˆ˜ ===
# PostgreSQL í•¨ìˆ˜ëŠ” SQLite ë§ˆì´ê·¸ë ˆì´ì…˜ìœ¼ë¡œ ì¸í•´ ì‚¬ìš© ì¤‘ë‹¨
# def get_db_connection():
#     return psycopg2.connect(
#         host=os.getenv("PG_HOST"),
#         port=os.getenv("PG_PORT"),
#         dbname=os.getenv("PG_DATABASE"),
#         user=os.getenv("PG_USER"),
#         password=os.getenv("PG_PASSWORD")
#     )

# SQLiteìš© DB ì—°ê²° - db_manager_sqlite.py ì‚¬ìš© ê¶Œì¥
def get_db_connection():
    """
    ë ˆê±°ì‹œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë”ë¯¸ í•¨ìˆ˜
    ì‹¤ì œë¡œëŠ” db_manager_sqlite.pyì˜ get_db_connection_context() ì‚¬ìš© ê¶Œì¥
    """
    import sqlite3
    return sqlite3.connect('./data/spock_local.db')

def setup_logger():
    """
    ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•˜ê³  ë¡œê±°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        logging.Logger: ì„¤ì •ëœ ë¡œê±° ê°ì²´
    """
    log_dir = "log"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    log_filename = safe_strftime(datetime.now(), "%Y%m%d") + "_spock.log"
    log_file_path = os.path.join(log_dir, log_filename)

    log_format = "%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s"

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì‚­ì œ
    if logger.hasHandlers():
        logger.handlers.clear()

    # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ (í„°ë¯¸ë„ ì¶œë ¥)
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(stream_handler)

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (íŒŒì¼ ì €ì¥)
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(file_handler)

    return logger

def setup_restricted_logger(logger_name: str = None):
    """
    ì œí•œëœ ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•˜ê³  ë¡œê±°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    íŠ¹ì • ë¡œê·¸ íŒŒì¼ ìƒì„± ì œí•œì„ ì ìš©í•©ë‹ˆë‹¤.

    Args:
        logger_name (str): ë¡œê±° ì´ë¦„ (Noneì´ë©´ ê¸°ë³¸ ë¡œê±°)

    Returns:
        logging.Logger: ì„¤ì •ëœ ë¡œê±° ê°ì²´
    """
    log_dir = "log"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    # ì œí•œëœ ë¡œê·¸ íŒŒì¼ëª… (spock.logë§Œ ìƒì„±)
    log_filename = safe_strftime(datetime.now(), "%Y%m%d") + "_spock.log"
    log_file_path = os.path.join(log_dir, log_filename)

    log_format = "%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d - %(message)s"

    # ë¡œê±° ìƒì„±
    if logger_name:
        logger = logging.getLogger(logger_name)
    else:
        logger = logging.getLogger()

    logger.setLevel(logging.INFO)

    # ğŸ”§ ì¤‘ë³µ ë¡œê¹… ë°©ì§€: propagation ë¹„í™œì„±í™”
    if logger_name:
        logger.propagate = False

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì‚­ì œ
    if logger.hasHandlers():
        logger.handlers.clear()

    # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ (í„°ë¯¸ë„ ì¶œë ¥)
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(stream_handler)

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (spock.logë§Œ ì‚¬ìš©)
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter(log_format))
    logger.addHandler(file_handler)

    return logger

def cleanup_old_log_files(retention_days: int = 7):
    """
    ì§€ì •ëœ ë³´ê´€ ê¸°ê°„ì„ ì´ˆê³¼í•œ ë¡œê·¸ íŒŒì¼ë“¤ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    
    Args:
        retention_days (int): ë¡œê·¸ íŒŒì¼ ë³´ê´€ ê¸°ê°„ (ì¼)
    
    Returns:
        dict: ì •ë¦¬ ê²°ê³¼ ì •ë³´
    """
    try:
        log_dir = "log"
        if not os.path.exists(log_dir):
            return {"status": "success", "message": "ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ", "deleted_count": 0}
        
        # í˜„ì¬ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë³´ê´€ ê¸°ê°„ ê³„ì‚°
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        deleted_count = 0
        error_count = 0
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ ê²€ì‚¬
        for filename in os.listdir(log_dir):
            file_path = os.path.join(log_dir, filename)
            
            # íŒŒì¼ì¸ì§€ í™•ì¸
            if not os.path.isfile(file_path):
                continue
            
            try:
                # íŒŒì¼ ìƒì„± ì‹œê°„ í™•ì¸
                file_creation_time = datetime.fromtimestamp(os.path.getctime(file_path))
                
                # ë³´ê´€ ê¸°ê°„ì„ ì´ˆê³¼í•œ íŒŒì¼ ì‚­ì œ
                if file_creation_time < cutoff_date:
                    os.remove(file_path)
                    deleted_count += 1
                    print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì‚­ì œ: {filename}")
                    
            except Exception as e:
                error_count += 1
                print(f"âš ï¸ ë¡œê·¸ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ({filename}): {e}")
        
        result = {
            "status": "success",
            "deleted_count": deleted_count,
            "error_count": error_count,
            "retention_days": retention_days,
            "cutoff_date": cutoff_date.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        if deleted_count > 0:
            print(f"âœ… ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ")
        else:
            print(f"â„¹ï¸ ì‚­ì œí•  ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ (ë³´ê´€ê¸°ê°„: {retention_days}ì¼)")
            
        return result
        
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            "deleted_count": 0,
            "error_count": 1
        }
        print(f"âŒ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return error_result

def get_log_file_info():
    """
    í˜„ì¬ ë¡œê·¸ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        dict: ë¡œê·¸ íŒŒì¼ ì •ë³´
    """
    try:
        log_dir = "log"
        if not os.path.exists(log_dir):
            return {"status": "error", "message": "ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"}
        
        log_files = []
        total_size = 0
        
        for filename in os.listdir(log_dir):
            file_path = os.path.join(log_dir, filename)
            
            if os.path.isfile(file_path):
                file_size = os.path.getsize(file_path)
                file_creation_time = datetime.fromtimestamp(os.path.getctime(file_path))
                
                log_files.append({
                    "filename": filename,
                    "size_bytes": file_size,
                    "size_mb": round(file_size / (1024 * 1024), 2),
                    "creation_time": file_creation_time.strftime("%Y-%m-%d %H:%M:%S"),
                    "age_days": (datetime.now() - file_creation_time).days
                })
                
                total_size += file_size
        
        # íŒŒì¼ í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬
        log_files.sort(key=lambda x: x["size_bytes"], reverse=True)
        
        return {
            "status": "success",
            "total_files": len(log_files),
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "files": log_files
        }
        
    except Exception as e:
        return {
            "status": "error", 
            "message": f"ë¡œê·¸ íŒŒì¼ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        }

# ë¡œê±° ì´ˆê¸°í™”
logger = setup_logger()

# === í˜„ì¬ê°€ ì•ˆì „ ì¡°íšŒ ===
def get_current_price_safe(ticker, retries=3, delay=0.3):
    """
    Stock market version - Uses KIS API instead of pyupbit

    Args:
        ticker: Stock ticker code (e.g., '005930' for Samsung Electronics)
        retries: Number of retry attempts
        delay: Delay between retries in seconds

    Returns:
        float: Current price of the stock
    """
    import time
    from modules.market_adapters import KoreaAdapter

    attempt = 0
    adapter = KoreaAdapter()

    while attempt < retries:
        try:
            price_data = adapter.get_current_price(ticker)
            if price_data is None:
                raise ValueError("No data returned")
            if isinstance(price_data, (int, float)):
                return price_data
            if isinstance(price_data, dict):
                if ticker in price_data:
                    return price_data[ticker]
                elif 'trade_price' in price_data:
                    return price_data['trade_price']
                else:
                    first_val = next(iter(price_data.values()), None)
                    if first_val is not None:
                        return first_val
            elif isinstance(price_data, list) and len(price_data) > 0:
                trade_price = price_data[0].get('trade_price')
                if trade_price is not None:
                    return trade_price
            raise ValueError(f"Unexpected data format: {price_data}")
        except Exception as e:
            logging.warning(f"âŒ {ticker} í˜„ì¬ê°€ ì¡°íšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            attempt += 1
            time.sleep(delay)
    return None

def retry_on_error(max_retries=3, delay=5):
    """ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ì‹œë„í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {str(e)}")
                        raise
                    logger.warning(f"âš ï¸ ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries})")
                    time.sleep(delay)
            return None
        return wrapper
    return decorator

def handle_api_error(e, context=""):
    """API ê´€ë ¨ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ê³  ë¡œê¹…í•©ë‹ˆë‹¤."""
    logger.error(f"âŒ API ì—ëŸ¬ ë°œìƒ ({context}): {str(e)}")
    if hasattr(e, 'response'):
        logger.error(f"ì‘ë‹µ ìƒíƒœ: {e.response.status_code}")
        logger.error(f"ì‘ë‹µ ë‚´ìš©: {e.response.text}")

def handle_db_error(e, context=""):
    """DB ê´€ë ¨ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ê³  ë¡œê¹…í•©ë‹ˆë‹¤."""
    logger.error(f"âŒ DB ì—ëŸ¬ ë°œìƒ ({context}): {str(e)}")

def handle_network_error(e, context=""):
    """ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ê³  ë¡œê¹…í•©ë‹ˆë‹¤."""
    logger.error(f"âŒ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë°œìƒ ({context}): {str(e)}")

def load_blacklist():
    """ë¸”ë™ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)"""
    try:
        blacklist_path = 'blacklist.json'
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(blacklist_path):
            logger.warning(f"âš ï¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {blacklist_path}")
            # ë¹ˆ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
            try:
                with open(blacklist_path, 'w', encoding='utf-8') as f:
                    json.dump({}, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… ë¹ˆ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì™„ë£Œ: {blacklist_path}")
                return {}
            except Exception as create_e:
                logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {create_e}")
                return {}
        
        # íŒŒì¼ ì½ê¸° ê¶Œí•œ í™•ì¸
        if not os.access(blacklist_path, os.R_OK):
            logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ê¶Œí•œ ì—†ìŒ: {blacklist_path}")
            return {}
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(blacklist_path)
        if file_size == 0:
            logger.warning(f"âš ï¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {blacklist_path}")
            return {}
        
        # JSON íŒŒì¼ ë¡œë“œ
        with open(blacklist_path, 'r', encoding='utf-8') as f:
            blacklist_data = json.load(f)
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        if not isinstance(blacklist_data, dict):
            logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: dict íƒ€ì…ì´ ì•„ë‹˜ (í˜„ì¬: {type(blacklist_data)})")
            return {}
        
        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë‚´ìš© ê²€ì¦
        valid_blacklist = {}
        invalid_entries = []
        
        for ticker, info in blacklist_data.items():
            # í‹°ì»¤ í˜•ì‹ ê²€ì¦
            if not isinstance(ticker, str) or not ticker.startswith('KRW-'):
                invalid_entries.append(f"{ticker}: ì˜ëª»ëœ í‹°ì»¤ í˜•ì‹")
                continue
            
            # ì •ë³´ êµ¬ì¡° ê²€ì¦
            if isinstance(info, dict):
                if 'reason' in info and 'added' in info:
                    valid_blacklist[ticker] = info
                else:
                    # êµ¬ì¡°ê°€ ë¶ˆì™„ì „í•œ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ë³´ì™„
                    valid_blacklist[ticker] = {
                        'reason': info.get('reason', 'ì‚¬ìœ  ì—†ìŒ'),
                        'added': info.get('added', datetime.now().isoformat())
                    }
                    logger.warning(f"âš ï¸ {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì •ë³´ ë¶ˆì™„ì „, ê¸°ë³¸ê°’ìœ¼ë¡œ ë³´ì™„")
            elif isinstance(info, str):
                # êµ¬ë²„ì „ í˜¸í™˜ì„± (ì‚¬ìœ ë§Œ ë¬¸ìì—´ë¡œ ì €ì¥ëœ ê²½ìš°)
                valid_blacklist[ticker] = {
                    'reason': info,
                    'added': datetime.now().isoformat()
                }
                logger.info(f"ğŸ”„ {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ êµ¬ë²„ì „ í˜•ì‹ ë³€í™˜")
            else:
                invalid_entries.append(f"{ticker}: ì˜ëª»ëœ ì •ë³´ í˜•ì‹")
        
        # ì˜ëª»ëœ í•­ëª© ë¡œê·¸
        if invalid_entries:
            logger.warning(f"âš ï¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì˜ëª»ëœ í•­ëª© {len(invalid_entries)}ê°œ ë°œê²¬:")
            for entry in invalid_entries[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                logger.warning(f"   - {entry}")
            if len(invalid_entries) > 5:
                logger.warning(f"   - ... ì™¸ {len(invalid_entries) - 5}ê°œ ë”")
        
        # ì„±ê³µ ë¡œê·¸
        logger.info(f"âœ… ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: {len(valid_blacklist)}ê°œ í•­ëª© (ìœ íš¨: {len(valid_blacklist)}, ë¬´íš¨: {len(invalid_entries)})")
        
        return valid_blacklist
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        logger.error(f"   íŒŒì¼ ìœ„ì¹˜: {os.path.abspath('blacklist.json')}")
        
        # ë°±ì—… íŒŒì¼ ìƒì„± ì‹œë„
        try:
            backup_path = f"blacklist_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            import shutil
            shutil.copy2('blacklist.json', backup_path)
            logger.info(f"ğŸ“‹ ì†ìƒëœ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ë°±ì—… ì™„ë£Œ: {backup_path}")
        except Exception as backup_e:
            logger.error(f"âŒ ë°±ì—… íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {backup_e}")
        
        return {}
        
    except PermissionError as e:
        logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì ‘ê·¼ ê¶Œí•œ ì˜¤ë¥˜: {e}")
        return {}
        
    except Exception as e:
        logger.error(f"âŒ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        logger.error(f"   íŒŒì¼ ìœ„ì¹˜: {os.path.abspath('blacklist.json')}")
        
        # ìƒì„¸ ë””ë²„ê¹… ì •ë³´
        try:
            import traceback
            logger.debug(f"ğŸ” ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\n{traceback.format_exc()}")
        except:
            pass
        
        return {}

# === UNUSED: ë¸”ë™ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬ í•¨ìˆ˜ë“¤ ===
# def add_to_blacklist(ticker: str, reason: str) -> bool:
#     """
#     ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— í‹°ì»¤ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
#     
#     Args:
#         ticker (str): ì¶”ê°€í•  í‹°ì»¤
#         reason (str): ì¶”ê°€ ì‚¬ìœ 
#         
#     Returns:
#         bool: ì„±ê³µ ì—¬ë¶€
#     """
#     try:
#         blacklist_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "blacklist.json")
#         blacklist = load_blacklist()
#         
#         # í˜„ì¬ UTC ì‹œê°„
#         now = datetime.now(pytz.UTC)
#         
#         # í‹°ì»¤ ì¶”ê°€
#         blacklist[ticker] = {
#             "reason": reason,
#             "added": now.isoformat()
#         }
#         
#         # JSON íŒŒì¼ ì €ì¥
#         with open(blacklist_path, 'w', encoding='utf-8') as f:
#             json.dump(blacklist, f, ensure_ascii=False, indent=2)
#             
#         logger.info(f"âœ… {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ ì™„ë£Œ (ì‚¬ìœ : {reason})")
#         return True
#         
#     except Exception as e:
#         logger.error(f"âŒ {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return False

# def remove_from_blacklist(ticker: str) -> bool:
#     """
#     ë¸”ë™ë¦¬ìŠ¤íŠ¸ì—ì„œ í‹°ì»¤ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
#     
#     Args:
#         ticker (str): ì œê±°í•  í‹°ì»¤
#         
#     Returns:
#         bool: ì„±ê³µ ì—¬ë¶€
#     """
#     try:
#         blacklist_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "blacklist.json")
#         blacklist = load_blacklist()
#         
#         if ticker not in blacklist:
#             logger.warning(f"âš ï¸ {ticker}ëŠ” ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ì—†ìŠµë‹ˆë‹¤.")
#             return False
#             
#         # í‹°ì»¤ ì œê±°
#         del blacklist[ticker]
#         
#         # JSON íŒŒì¼ ì €ì¥
#         with open(blacklist_path, 'w', encoding='utf-8') as f:
#             json.dump(blacklist, f, ensure_ascii=False, indent=2)
#             
#         logger.info(f"âœ… {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œê±° ì™„ë£Œ")
#         return True
#         
#     except Exception as e:
#         logger.error(f"âŒ {ticker} ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì œê±° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return False

# === í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ì •ê·œí™” ===
COLUMN_MAPPING = {
    'static_indicators': {
        # í”¼ë²— í¬ì¸íŠ¸ ë§¤í•‘
        'resistance_1': 'r1',
        'resistance_2': 'r2', 
        'resistance_3': 'r3',
        'support_1': 's1',
        'support_2': 's2',
        'support_3': 's3',
        
        # í”¼ë³´ë‚˜ì¹˜ ë§¤í•‘ (ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)
        'fib_382': 'fibo_382',
        'fib_618': 'fibo_618',
        
        # ê¸°íƒ€ ë§¤í•‘
        'ma200': 'ma_200',
        'volume_ratio': 'volume_change_7_30'
    },
    'ohlcv': {
        # OHLCV ê¸°ë³¸ ì»¬ëŸ¼ ë§¤í•‘
        'open_price': 'open',
        'high_price': 'high',
        'low_price': 'low',
        'close_price': 'close',
        'trading_volume': 'volume',
        'trade_date': 'date',
        
        # ì—­í˜¸í™˜ì„±ì„ ìœ„í•œ MACD ë§¤í•‘
        'macd_hist': 'macd_histogram'
    }
}

def apply_column_mapping(df, table_name):
    """
    DataFrameì˜ ì»¬ëŸ¼ëª…ì„ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë§¤í•‘í•©ë‹ˆë‹¤.
    
    Args:
        df (pd.DataFrame): ë§¤í•‘í•  DataFrame
        table_name (str): ëŒ€ìƒ í…Œì´ë¸”ëª…
        
    Returns:
        pd.DataFrame: ì»¬ëŸ¼ëª…ì´ ë§¤í•‘ëœ DataFrame
    """
    if table_name not in COLUMN_MAPPING:
        logger.debug(f"âš ï¸ {table_name} í…Œì´ë¸”ì— ëŒ€í•œ ì»¬ëŸ¼ ë§¤í•‘ì´ ì •ì˜ë˜ì§€ ì•ŠìŒ")
        return df
        
    mapping = COLUMN_MAPPING[table_name]
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ë§¤í•‘
    columns_to_rename = {}
    for old_col, new_col in mapping.items():
        if old_col in df.columns:
            columns_to_rename[old_col] = new_col
            
    if columns_to_rename:
        df_mapped = df.rename(columns=columns_to_rename)
        logger.debug(f"âœ… {table_name} í…Œì´ë¸” ì»¬ëŸ¼ ë§¤í•‘ ì ìš©: {list(columns_to_rename.keys())} â†’ {list(columns_to_rename.values())}")
        return df_mapped
    else:
        logger.debug(f"â„¹ï¸ {table_name} í…Œì´ë¸”ì— ë§¤í•‘í•  ì»¬ëŸ¼ì´ ì—†ìŒ")
        return df

def validate_ticker_filtering_system():
    """
    í‹°ì»¤ í•„í„°ë§ ì‹œìŠ¤í…œì˜ ë‹¤ì¸µ ê²€ì¦ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
    
    ê²€ì¦ í•­ëª©:
    1. tickers í…Œì´ë¸”ì˜ is_active ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€
    2. ë¸”ë™ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì ‘ê·¼ ê°€ëŠ¥ì„±
    3. ë‘ í•„í„°ë§ ë°©ì‹ì˜ ê²°ê³¼ ë¹„êµ
    """
    results = {
        "is_active_available": False,
        "blacklist_available": False,
        "filtering_consistency": False,
        "active_count": 0,
        "blacklist_filtered_count": 0,
        "consistency_rate": 0.0
    }
    
    try:
        # 1. is_active ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT column_name FROM information_schema.columns 
            WHERE table_name = 'tickers' AND column_name = 'is_active'
        """)
        results["is_active_available"] = len(cursor.fetchall()) > 0
        
        # 2. ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì ‘ê·¼ í™•ì¸
        blacklist = load_blacklist()
        results["blacklist_available"] = blacklist is not None
        
        # 3. ë‘ ë°©ì‹ ê²°ê³¼ ë¹„êµ (is_active ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ)
        if results["is_active_available"]:
            cursor.execute("SELECT ticker FROM tickers WHERE is_active = true")
            active_tickers = {row[0] for row in cursor.fetchall()}
            results["active_count"] = len(active_tickers)
            
            cursor.execute("SELECT ticker FROM tickers")
            all_tickers = {row[0] for row in cursor.fetchall()}
            
            blacklist_filtered = all_tickers - set(blacklist if blacklist else [])
            results["blacklist_filtered_count"] = len(blacklist_filtered)
            
            # ê²°ê³¼ ì¼ì¹˜ë„ ê²€ì‚¬
            overlap = len(active_tickers & blacklist_filtered)
            total = len(active_tickers | blacklist_filtered)
            consistency_rate = overlap / total if total > 0 else 0
            results["consistency_rate"] = consistency_rate
            
            results["filtering_consistency"] = consistency_rate > 0.8
        
        cursor.close()
        conn.close()
        return results
        
    except Exception as e:
        logger.error(f"âŒ í‹°ì»¤ í•„í„°ë§ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return results

def get_safe_ohlcv_columns():
    """
    ì‹¤ì œ ohlcv í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë“¤ì„ ë°˜í™˜í•˜ëŠ” ì•ˆì „í•œ í•¨ìˆ˜
    
    Returns:
        list: ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ohlcv í…Œì´ë¸” ì»¬ëŸ¼ ëª©ë¡
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'ohlcv' 
            AND table_schema = 'public'
            ORDER BY ordinal_position
        """)
        
        columns = [row[0] for row in cursor.fetchall()]
        cursor.close()
        conn.close()
        
        logger = setup_logger()
        logger.debug(f"ğŸ“Š get_safe_ohlcv_columns() ì¡°íšŒ ì„±ê³µ: {len(columns)}ê°œ ì»¬ëŸ¼")
        
        return columns
        
    except Exception as e:
        logger = setup_logger()
        logger.error(f"âŒ ohlcv ì»¬ëŸ¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ ì»¬ëŸ¼ ë°˜í™˜
        return ['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']

def validate_db_schema_and_indicators():
    """
    ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆì™€ ì§€í‘œ ë°ì´í„° í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜
    
    ì‘ì—… ë‚´ìš©:
    1. .env íŒŒì¼ì˜ DB ì—°ê²° ì •ë³´ í™•ì¸
    2. ohlcv í…Œì´ë¸”ì˜ ì‹¤ì œ ì»¬ëŸ¼ êµ¬ì¡°ë¥¼ ì¡°íšŒí•˜ì—¬ ì¡´ì¬í•˜ëŠ” ë™ì  ì§€í‘œ ì»¬ëŸ¼ë“¤ë§Œ í™•ì¸
    3. static_indicators í…Œì´ë¸”ì— ìˆëŠ” ì •ì  ì§€í‘œ ì»¬ëŸ¼ë“¤ í™•ì¸
    4. ì‹¤ì œ DBì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œì„ ëŒ€ìƒìœ¼ë¡œ NULL ê°’ ë¹„ìœ¨ í™•ì¸
    5. ê²€ì¦ ê²°ê³¼ë¥¼ ìƒì„¸íˆ ë¡œê¹…
    
    Returns:
        dict: ê²€ì¦ ê²°ê³¼ ì •ë³´
    """
    logger = setup_logger()
    
    try:
        logger.info("ğŸ” DB ìŠ¤í‚¤ë§ˆ ë° ì§€í‘œ êµ¬ì¡° ê²€ì¦ ì‹œì‘")
        
        # 1. DB ì—°ê²° ì •ë³´ í™•ì¸
        required_env_vars = ['PG_HOST', 'PG_PORT', 'PG_DATABASE', 'PG_USER', 'PG_PASSWORD']
        missing_vars = []
        
        for var in required_env_vars:
            value = os.getenv(var)
            if not value:
                missing_vars.append(var)
            else:
                # ë¹„ë°€ë²ˆí˜¸ëŠ” ë§ˆìŠ¤í‚¹
                if var == 'PG_PASSWORD':
                    logger.info(f"âœ… {var}: {'*' * len(value)}")
                else:
                    logger.info(f"âœ… {var}: {value}")
        
        if missing_vars:
            logger.error(f"âŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: {missing_vars}")
            return {'status': 'error', 'error': f'Missing environment variables: {missing_vars}'}
        
        # 2. DB ì—°ê²°
        conn = get_db_connection()
        cursor = conn.cursor()
        
        try:
            # 3. ohlcv í…Œì´ë¸” ì‹¤ì œ ì»¬ëŸ¼ ì¡°íšŒ
            logger.info("ğŸ” ohlcv í…Œì´ë¸” ì»¬ëŸ¼ êµ¬ì¡° ì¡°íšŒ ì¤‘...")
            cursor.execute("""
                SELECT column_name, data_type, is_nullable 
                FROM information_schema.columns 
                WHERE table_name = 'ohlcv' 
                AND table_schema = 'public'
                ORDER BY ordinal_position
            """)
            
            ohlcv_columns_info = cursor.fetchall()
            actual_ohlcv_columns = []
            
            for col_name, data_type, is_nullable in ohlcv_columns_info:
                actual_ohlcv_columns.append(col_name)
                null_status = "NOT NULL" if is_nullable == "NO" else "NULL"
                logger.info(f"   ğŸ“Š {col_name}: {data_type} ({null_status})")
            
            # 4. static_indicators í…Œì´ë¸” ì‹¤ì œ ì»¬ëŸ¼ ì¡°íšŒ
            logger.info("ğŸ” static_indicators í…Œì´ë¸” ì»¬ëŸ¼ êµ¬ì¡° ì¡°íšŒ ì¤‘...")
            cursor.execute("""
                SELECT column_name, data_type, is_nullable 
                FROM information_schema.columns 
                WHERE table_name = 'static_indicators' 
                AND table_schema = 'public'
                ORDER BY ordinal_position
            """)
            
            static_columns_info = cursor.fetchall()
            actual_static_columns = []
            
            for col_name, data_type, is_nullable in static_columns_info:
                actual_static_columns.append(col_name)
                null_status = "NOT NULL" if is_nullable == "NO" else "NULL"
                logger.info(f"   ğŸ“Š {col_name}: {data_type} ({null_status})")
            
            # 5. ë™ì  ì§€í‘œ ì»¬ëŸ¼ ì¤‘ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ í•„í„°ë§
            expected_dynamic_indicators = [
                'fibo_618', 'fibo_382', 'ht_trendline', 'ma_50', 'ma_200', 
                'bb_upper', 'bb_lower', 'donchian_high', 'donchian_low', 
                'macd_histogram', 'rsi_14', 'volume_20ma', 'stoch_k', 'stoch_d', 'cci'
            ]
            
            existing_dynamic_indicators = [col for col in expected_dynamic_indicators if col in actual_ohlcv_columns]
            missing_dynamic_indicators = [col for col in expected_dynamic_indicators if col not in actual_ohlcv_columns]
            
            logger.info(f"ğŸ“Š ë™ì  ì§€í‘œ í˜„í™©:")
            logger.info(f"   â€¢ ì¡´ì¬í•˜ëŠ” ë™ì  ì§€í‘œ: {len(existing_dynamic_indicators)}ê°œ")
            for col in existing_dynamic_indicators:
                logger.info(f"     âœ… {col}")
            
            if missing_dynamic_indicators:
                logger.warning(f"   â€¢ ëˆ„ë½ëœ ë™ì  ì§€í‘œ: {len(missing_dynamic_indicators)}ê°œ")
                for col in missing_dynamic_indicators:
                    logger.warning(f"     âŒ {col}")
            
            # 6. ê° ì§€í‘œì˜ NULL ë¹„ìœ¨ í™•ì¸ (ì˜ˆ: 005930 Samsung Electronics ê¸°ì¤€)
            logger.info("ğŸ” ë™ì  ì§€í‘œ NULL ë¹„ìœ¨ ê²€ì¦ ì¤‘...")
            null_analysis = {}
            
            for indicator in existing_dynamic_indicators:
                try:
                    cursor.execute(f"""
                        SELECT
                            COUNT(*) as total,
                            COUNT({indicator}) as non_null,
                            COUNT(*) - COUNT({indicator}) as null_count
                        FROM ohlcv_data
                        WHERE ticker = '005930'
                        AND date >= date('now', '-30 days')
                    """)
                    
                    result = cursor.fetchone()
                    if result and result[0] > 0:
                        total, non_null, null_count = result
                        null_ratio = (null_count / total) * 100 if total > 0 else 100
                        
                        null_analysis[indicator] = {
                            'total': total,
                            'non_null': non_null, 
                            'null_count': null_count,
                            'null_ratio': null_ratio
                        }
                        
                        if null_ratio > 80:
                            logger.warning(f"  âš ï¸ {indicator}: NULL ë¹„ìœ¨ {null_ratio:.1f}% ({null_count}/{total})")
                        elif null_ratio > 50:
                            logger.info(f"  ğŸ”¶ {indicator}: NULL ë¹„ìœ¨ {null_ratio:.1f}% ({null_count}/{total})")
                        else:
                            logger.info(f"  âœ… {indicator}: NULL ë¹„ìœ¨ {null_ratio:.1f}% ({null_count}/{total})")
                    else:
                        logger.warning(f"  âš ï¸ {indicator}: ë°ì´í„° ì—†ìŒ")
                        null_analysis[indicator] = {'error': 'no_data'}
                        
                except Exception as e:
                    logger.error(f"  âŒ {indicator} NULL ë¹„ìœ¨ í™•ì¸ ì‹¤íŒ¨: {e}")
                    null_analysis[indicator] = {'error': str(e)}
            
            # 7. static_indicators ì»¬ëŸ¼ ê²€ì¦
            expected_static_indicators = [
                'nvt_relative', 'volume_change_7_30', 'price', 
                'high_60', 'low_60', 'pivot', 's1', 'r1', 'resistance', 
                'support', 'atr', 'adx', 'supertrend_signal'
            ]
            
            existing_static_indicators = [col for col in expected_static_indicators if col in actual_static_columns]
            missing_static_indicators = [col for col in expected_static_indicators if col not in actual_static_columns]
            
            logger.info(f"ğŸ“Š ì •ì  ì§€í‘œ í˜„í™©:")
            logger.info(f"   â€¢ ì¡´ì¬í•˜ëŠ” ì •ì  ì§€í‘œ: {len(existing_static_indicators)}ê°œ")
            for col in existing_static_indicators:
                logger.info(f"     âœ… {col}")
            
            if missing_static_indicators:
                logger.warning(f"   â€¢ ëˆ„ë½ëœ ì •ì  ì§€í‘œ: {len(missing_static_indicators)}ê°œ")
                for col in missing_static_indicators:
                    logger.warning(f"     âŒ {col}")
            
            # 8. ê²€ì¦ ê²°ê³¼ ìš”ì•½
            logger.info("ğŸ“Š DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê²°ê³¼ ìš”ì•½:")
            logger.info(f"   â€¢ ohlcv í…Œì´ë¸” ì»¬ëŸ¼ ìˆ˜: {len(actual_ohlcv_columns)}")
            logger.info(f"   â€¢ static_indicators í…Œì´ë¸” ì»¬ëŸ¼ ìˆ˜: {len(actual_static_columns)}")
            logger.info(f"   â€¢ ì¡´ì¬í•˜ëŠ” ë™ì  ì§€í‘œ: {len(existing_dynamic_indicators)}/{len(expected_dynamic_indicators)}")
            logger.info(f"   â€¢ ì¡´ì¬í•˜ëŠ” ì •ì  ì§€í‘œ: {len(existing_static_indicators)}/{len(expected_static_indicators)}")
            
            total_missing = len(missing_dynamic_indicators) + len(missing_static_indicators)
            if total_missing > 0:
                logger.warning(f"   â€¢ ì´ ëˆ„ë½ ì»¬ëŸ¼ ìˆ˜: {total_missing}")
            
            return {
                'status': 'success',
                'ohlcv_columns': actual_ohlcv_columns,
                'static_columns': actual_static_columns,
                'existing_dynamic_indicators': existing_dynamic_indicators,
                'missing_dynamic_indicators': missing_dynamic_indicators,
                'existing_static_indicators': existing_static_indicators,
                'missing_static_indicators': missing_static_indicators,
                'null_analysis': null_analysis,
                'total_missing_columns': total_missing
            }
            
        finally:
            cursor.close()
            conn.close()
            
    except Exception as e:
        logger.error(f"âŒ DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return {'status': 'error', 'error': str(e)}
    
def safe_float_convert(value, default=0.0, context=""):
    """
    ëª¨ë“  íƒ€ì…ì˜ ê°’ì„ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ ì¶”ê°€)
    
    Args:
        value: ë³€í™˜í•  ê°’
        default (float): ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        context (str): ë¡œê¹…ìš© ì»¨í…ìŠ¤íŠ¸
        
    Returns:
        float: ë³€í™˜ëœ ê°’ ë˜ëŠ” ê¸°ë³¸ê°’
    """
    global _conversion_stats
    
    # ì „ì—­ ë³€ìˆ˜ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì´ˆê¸°í™”
    if '_conversion_stats' not in globals():
        globals()['_conversion_stats'] = {
            "total_calls": 0,
            "successful_conversions": 0,
            "conversion_failures": 0,
            "datetime_detections": 0
        }
    
    _conversion_stats["total_calls"] += 1
    
    if value is None:
        _conversion_stats["successful_conversions"] += 1
        return default
        
    # datetime ê³„ì—´ ê°ì²´ëŠ” ë³€í™˜í•˜ì§€ ì•ŠìŒ
    if isinstance(value, (datetime, date)):
        _conversion_stats["datetime_detections"] += 1
        logger.warning(f"âš ï¸ {context} datetime ê°ì²´ë¥¼ floatë¡œ ë³€í™˜ ì‹œë„: {value} -> {default}")
        return default
        
    # pandas Timestampë„ ì²´í¬
    if hasattr(value, '__class__') and 'pandas' in str(type(value)):
        _conversion_stats["datetime_detections"] += 1
        logger.warning(f"âš ï¸ {context} pandas ê°ì²´ë¥¼ floatë¡œ ë³€í™˜ ì‹œë„: {value} -> {default}")
        return default
        
    try:
        result = float(value)
        _conversion_stats["successful_conversions"] += 1
        return result
    except (ValueError, TypeError) as e:
        _conversion_stats["conversion_failures"] += 1
        logger.warning(f"âš ï¸ {context} float ë³€í™˜ ì‹¤íŒ¨: {value} (íƒ€ì…: {type(value)}) -> {default}")
        return default

def get_conversion_stats():
    """
    safe_float_convert í•¨ìˆ˜ì˜ ë³€í™˜ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        dict: ë³€í™˜ í†µê³„ ì •ë³´
    """
    global _conversion_stats
    if '_conversion_stats' not in globals():
        return {
            "total_calls": 0,
            "successful_conversions": 0,
            "conversion_failures": 0,
            "datetime_detections": 0
        }
    return _conversion_stats.copy()

def reset_conversion_stats():
    """
    safe_float_convert í•¨ìˆ˜ì˜ ë³€í™˜ í†µê³„ë¥¼ ë¦¬ì…‹í•©ë‹ˆë‹¤.
    """
    global _conversion_stats
    _conversion_stats = {
        "total_calls": 0,
        "successful_conversions": 0,
        "conversion_failures": 0,
        "datetime_detections": 0
    }


def check_market_hours(target_date: Optional[date] = None) -> Dict[str, any]:
    """
    Check Korean stock market hours (KOSPI/KOSDAQ)

    Trading Hours (KST = UTC+9):
    - Pre-market: 08:00-09:00
    - Regular trading: 09:00-15:30
    - After-hours: 15:30-08:00 (next day)
    - Closed: Weekends + Korean holidays

    Args:
        target_date: Date to check (default: today in KST)

    Returns:
        {
            'is_market_open': bool,
            'status': 'market_open' | 'pre_market' | 'after_hours' | 'market_closed',
            'current_time_kst': datetime,
            'market_date': date,
            'next_open': datetime,
            'next_close': datetime
        }
    """
    # KST timezone
    kst = pytz.timezone('Asia/Seoul')

    # Current time in KST
    now_kst = datetime.now(kst)
    current_date = target_date or now_kst.date()
    current_time = now_kst.time()

    # Market hours definition
    pre_market_start = datetime.strptime("08:00", "%H:%M").time()
    market_open = datetime.strptime("09:00", "%H:%M").time()
    market_close = datetime.strptime("15:30", "%H:%M").time()

    # Check if weekend
    is_weekend = current_date.weekday() >= 5  # 5=Saturday, 6=Sunday

    # Check if holiday (load from config if available)
    is_holiday = _check_korean_holiday(current_date)

    # Determine market status
    if is_weekend or is_holiday:
        # Market closed
        status = 'market_closed'
        is_market_open = False

        # Calculate next open time (next business day 09:00)
        next_open_date = _get_next_business_day(current_date)
        next_open = datetime.combine(next_open_date, market_open)
        next_open = kst.localize(next_open)
        next_close = datetime.combine(next_open_date, market_close)
        next_close = kst.localize(next_close)

    elif pre_market_start <= current_time < market_open:
        # Pre-market
        status = 'pre_market'
        is_market_open = False
        next_open = datetime.combine(current_date, market_open)
        next_open = kst.localize(next_open)
        next_close = datetime.combine(current_date, market_close)
        next_close = kst.localize(next_close)

    elif market_open <= current_time < market_close:
        # Regular trading hours
        status = 'market_open'
        is_market_open = True
        next_open = datetime.combine(current_date, market_open)
        next_open = kst.localize(next_open)
        next_close = datetime.combine(current_date, market_close)
        next_close = kst.localize(next_close)

    else:
        # After-hours
        status = 'after_hours'
        is_market_open = False

        # Next open is next business day
        next_open_date = _get_next_business_day(current_date)
        next_open = datetime.combine(next_open_date, market_open)
        next_open = kst.localize(next_open)
        next_close = datetime.combine(next_open_date, market_close)
        next_close = kst.localize(next_close)

    return {
        'is_market_open': is_market_open,
        'status': status,
        'current_time_kst': now_kst,
        'market_date': current_date,
        'next_open': next_open,
        'next_close': next_close,
        'is_weekend': is_weekend,
        'is_holiday': is_holiday
    }


def _check_korean_holiday(check_date: date) -> bool:
    """
    Check if a date is a Korean public holiday

    Args:
        check_date: Date to check

    Returns:
        True if holiday, False otherwise
    """
    # Korean public holidays 2025 (update annually)
    holidays_2025 = [
        date(2025, 1, 1),   # New Year's Day
        date(2025, 1, 28),  # Lunar New Year's Eve
        date(2025, 1, 29),  # Lunar New Year
        date(2025, 1, 30),  # Lunar New Year
        date(2025, 3, 1),   # Independence Movement Day
        date(2025, 5, 5),   # Children's Day
        date(2025, 5, 6),   # Buddha's Birthday
        date(2025, 6, 6),   # Memorial Day
        date(2025, 8, 15),  # Liberation Day
        date(2025, 9, 28),  # Chuseok Eve
        date(2025, 9, 29),  # Chuseok
        date(2025, 9, 30),  # Chuseok
        date(2025, 10, 3),  # National Foundation Day
        date(2025, 10, 9),  # Hangeul Day
        date(2025, 12, 25), # Christmas Day
    ]

    # Try to load from config file if available
    config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)),
                               'config', 'market_schedule.json')

    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                year = check_date.year
                if str(year) in config.get('korean_holidays', {}):
                    holiday_strings = config['korean_holidays'][str(year)]
                    config_holidays = [datetime.strptime(h, '%Y-%m-%d').date()
                                     for h in holiday_strings]
                    return check_date in config_holidays
        except Exception as e:
            # Fallback to hardcoded holidays if config loading fails
            pass

    # Use hardcoded holidays as fallback
    return check_date in holidays_2025


def _get_next_business_day(current_date: date) -> date:
    """
    Get next business day (excluding weekends and holidays)

    Args:
        current_date: Starting date

    Returns:
        Next business day
    """
    next_date = current_date + timedelta(days=1)

    # Loop until we find a business day (max 10 days to prevent infinite loop)
    for _ in range(10):
        # Check if weekend
        if next_date.weekday() < 5:  # Monday=0, Friday=4
            # Check if holiday
            if not _check_korean_holiday(next_date):
                return next_date

        # Not a business day, try next day
        next_date += timedelta(days=1)

    # Fallback: return original next date if loop exhausted
    return current_date + timedelta(days=1)